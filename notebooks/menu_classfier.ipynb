{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "690a7e01",
   "metadata": {},
   "source": [
    "# Menu Classifier\n",
    "\n",
    "This notebook shows the ML model to detect Menu pictures\n",
    "\n",
    "## Data from Yelp\n",
    "\n",
    "We use a sample of aproximately 65000+ images of yelp dataset. \n",
    "This dataset contains over 65,000 images from Yelp, categorized into five classes: Food, Inside, Outside, Drink, and Menu. The dataset is split into training and test sets, with the training set comprising 95% of the images and the test set comprising 5% of the images. The metadata includes photo IDs, business IDs, captions, and labels for each image.\n",
    "\n",
    "### Dataset Structure\n",
    "\n",
    "The metadata is provided in a CSV file with the following columns:\n",
    "\n",
    "**photo_id**: Unique identifier for each photo.\\\n",
    "**business_id**: Unique identifier for the business associated with the photo./\n",
    "**caption**: Description or caption associated with the photo./\n",
    "**label**: Class label of the photo (Food, Inside, Outside, Drink, Menu).\n",
    "### Images\n",
    "The images are stored in separate directories for the training and test sets. Each directory contains subdirectories for each class label."
   ]
  },
  {
   "cell_type": "raw",
   "id": "23aba664",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Menu_Classifier/\n",
    "├── data_processed/\n",
    "│   ├── train/\n",
    "│   │   ├── menu/\n",
    "│   │   └── non_menu/\n",
    "│   └── test/\n",
    "│       ├── menu/\n",
    "│       └── non_menu/\n",
    "├── notebooks/\n",
    "│   └── menu_classifier.ipynb\n",
    "├── src/\n",
    "│   └── utils.py\n",
    "├── requirements.txt\n",
    "└── README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652e3be7",
   "metadata": {},
   "source": [
    "#### 1 - Initial Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654311e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import os\n",
    "import csv  \n",
    "import shutil\n",
    "from datetime import datetime  \n",
    "\n",
    "# Image processing\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.applications import VGG16, MobileNetV2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Define paths\n",
    "train_dir = \"C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/train\"\n",
    "test_dir = \"C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/test\"\n",
    "\n",
    "# Only these extensions will be treated as images\n",
    "image_extensions = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2534155",
   "metadata": {},
   "source": [
    "### 2 - Data Loading and Preprocessing\n",
    "We use Keras ImageDataGenerator to load and preprocess images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767cbaeb",
   "metadata": {},
   "source": [
    "#### 2.1 - Clean out corrupted/non-image files\n",
    "Keras’ flow_from_directory will try to open every file in your menu/non_menu folders. If it encounters a file that isn’t a valid JPEG/PNG (e.g. a corrupted image, or a stray system file like Thumbs.db), PIL raises UnidentifiedImageError."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0438650c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the Reusable Cleanup Function\n",
    "def segregate_corrupted_images(\n",
    "    root_dir,\n",
    "    corrupt_dir_name=\"corrupted\",\n",
    "    log_filename=\"corruption_log.csv\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Moves unreadable or non-image files into a 'corrupted' folder and logs each move with:\n",
    "      - original path\n",
    "      - new path\n",
    "      - file size (bytes)\n",
    "      - last-modified timestamp (ISO)\n",
    "    Q1: Log extended with size & timestamp.\n",
    "    Q3: Encapsulated for reuse across projects.\n",
    "    \"\"\"\n",
    "    # Build paths\n",
    "    corrupt_dir = os.path.join(root_dir, corrupt_dir_name)  \n",
    "    os.makedirs(corrupt_dir, exist_ok=True)  # Create if missing\n",
    "    log_path = os.path.join(root_dir, log_filename)  \n",
    "\n",
    "    # Open CSV and write header\n",
    "    with open(log_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as log_file:\n",
    "        writer = csv.writer(log_file)  \n",
    "        writer.writerow([\n",
    "            \"original_path\",\n",
    "            \"moved_path\",\n",
    "            \"file_size_bytes\",\n",
    "            \"last_modified\"\n",
    "        ])\n",
    "\n",
    "        # Walk through all files under root_dir\n",
    "        for subdir, _, files in os.walk(root_dir):\n",
    "            # Skip any files already in the corrupted folder\n",
    "            if corrupt_dir in subdir:\n",
    "                continue\n",
    "\n",
    "            for fname in files:\n",
    "                # Skip the log file itself\n",
    "                if fname == log_filename:\n",
    "                    continue\n",
    "                # Only attempt images with allowed extensions\n",
    "                if not fname.lower().endswith(image_extensions):\n",
    "                    continue\n",
    "\n",
    "                file_path = os.path.join(subdir, fname)  # Full file path\n",
    "                try:\n",
    "                    # Attempt to open and verify image integrity\n",
    "                    img = Image.open(file_path)  \n",
    "                    img.verify()  \n",
    "                except Exception:\n",
    "                    # On failure, gather diagnostics\n",
    "                    file_size = os.path.getsize(file_path)  \n",
    "                    mtime = os.path.getmtime(file_path)  \n",
    "                    last_mod = datetime.fromtimestamp(mtime).isoformat()  \n",
    "\n",
    "                    # Determine destination and ensure its folder exists\n",
    "                    rel_path = os.path.relpath(file_path, root_dir)\n",
    "                    dest_path = os.path.join(corrupt_dir, rel_path)\n",
    "                    os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n",
    "\n",
    "                    # Move the bad file and log its details\n",
    "                    shutil.move(file_path, dest_path)  \n",
    "                    writer.writerow([\n",
    "                        file_path,\n",
    "                        dest_path,\n",
    "                        file_size,\n",
    "                        last_mod\n",
    "                    ])\n",
    "                    print(f\"Moved corrupted file: {file_path} → {dest_path}\")\n",
    "\n",
    "    print(f\"Corruption log written to: {log_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50c65bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved corrupted file: C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/train\\non_menu\\1wd_eyhMrTqUmicDmn4_Kw.jpg → C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/train\\corrupted\\non_menu\\1wd_eyhMrTqUmicDmn4_Kw.jpg\n",
      "Moved corrupted file: C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/train\\non_menu\\2S78q98b_VpBD7vkrDE5-A.jpg → C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/train\\corrupted\\non_menu\\2S78q98b_VpBD7vkrDE5-A.jpg\n",
      "Moved corrupted file: C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/train\\non_menu\\43fHlHSYQ_79OBJW1aVUxA.jpg → C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/train\\corrupted\\non_menu\\43fHlHSYQ_79OBJW1aVUxA.jpg\n",
      "Moved corrupted file: C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/train\\non_menu\\9BvYOtforBBP6MvvDogtmw.jpg → C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/train\\corrupted\\non_menu\\9BvYOtforBBP6MvvDogtmw.jpg\n",
      "Moved corrupted file: C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/train\\non_menu\\AMSyCOP3-Eb_ivNA8w1Vhw.jpg → C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/train\\corrupted\\non_menu\\AMSyCOP3-Eb_ivNA8w1Vhw.jpg\n",
      "Moved corrupted file: C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/train\\non_menu\\ARwqGQZaT0p-XpYYjMXgQg.jpg → C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/train\\corrupted\\non_menu\\ARwqGQZaT0p-XpYYjMXgQg.jpg\n",
      "Moved corrupted file: C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/train\\non_menu\\IUsKp87a-v9Yhx6Ftg1m5A.jpg → C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/train\\corrupted\\non_menu\\IUsKp87a-v9Yhx6Ftg1m5A.jpg\n",
      "Moved corrupted file: C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/train\\non_menu\\JG5s_bvRF1cSWf1fk9lTbw.jpg → C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/train\\corrupted\\non_menu\\JG5s_bvRF1cSWf1fk9lTbw.jpg\n",
      "Moved corrupted file: C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/train\\non_menu\\JZZ716oX6_MqH6L_MkWK-A.jpg → C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/train\\corrupted\\non_menu\\JZZ716oX6_MqH6L_MkWK-A.jpg\n",
      "Moved corrupted file: C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/train\\non_menu\\ke4ohxa93GJz0KH9H2kwsQ.jpg → C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/train\\corrupted\\non_menu\\ke4ohxa93GJz0KH9H2kwsQ.jpg\n",
      "Moved corrupted file: C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/train\\non_menu\\kjMBhxBXOUE7SSUQb-YQbw.jpg → C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/train\\corrupted\\non_menu\\kjMBhxBXOUE7SSUQb-YQbw.jpg\n",
      "Moved corrupted file: C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/train\\non_menu\\lrfy4UVIWtj0xwboLgUreQ.jpg → C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/train\\corrupted\\non_menu\\lrfy4UVIWtj0xwboLgUreQ.jpg\n",
      "Moved corrupted file: C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/train\\non_menu\\l_rMdwgrvjm2PyHyXBcBTw.jpg → C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/train\\corrupted\\non_menu\\l_rMdwgrvjm2PyHyXBcBTw.jpg\n",
      "Moved corrupted file: C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/train\\non_menu\\NfayhoTudVJQsEF-XlPyjw.jpg → C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/train\\corrupted\\non_menu\\NfayhoTudVJQsEF-XlPyjw.jpg\n",
      "Moved corrupted file: C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/train\\non_menu\\OK6HsALzFcBAUlrroKHZGg.jpg → C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/train\\corrupted\\non_menu\\OK6HsALzFcBAUlrroKHZGg.jpg\n",
      "Moved corrupted file: C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/train\\non_menu\\PFD3ykdI1WVhvZ8IX4PmLQ.jpg → C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/train\\corrupted\\non_menu\\PFD3ykdI1WVhvZ8IX4PmLQ.jpg\n",
      "Moved corrupted file: C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/train\\non_menu\\PjfJoBrEFgDrxiJy8nyatA.jpg → C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/train\\corrupted\\non_menu\\PjfJoBrEFgDrxiJy8nyatA.jpg\n",
      "Moved corrupted file: C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/train\\non_menu\\QhATx1B1n8uf8C6siMNTfA.jpg → C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/train\\corrupted\\non_menu\\QhATx1B1n8uf8C6siMNTfA.jpg\n",
      "Moved corrupted file: C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/train\\non_menu\\RhC7TNmFvbR9GWrlrl5dsA.jpg → C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/train\\corrupted\\non_menu\\RhC7TNmFvbR9GWrlrl5dsA.jpg\n",
      "Moved corrupted file: C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/train\\non_menu\\RLtBKD2rlfTaELWejmLBCA.jpg → C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/train\\corrupted\\non_menu\\RLtBKD2rlfTaELWejmLBCA.jpg\n",
      "Moved corrupted file: C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/train\\non_menu\\tSHz7RzlgceAItRejZ396A.jpg → C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/train\\corrupted\\non_menu\\tSHz7RzlgceAItRejZ396A.jpg\n",
      "Moved corrupted file: C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/train\\non_menu\\XX6ujA9CcB5s9y9wCy67-Q.jpg → C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/train\\corrupted\\non_menu\\XX6ujA9CcB5s9y9wCy67-Q.jpg\n",
      "Moved corrupted file: C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/train\\non_menu\\ytJ4lihJrvyzMMRG-WwDNw.jpg → C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/train\\corrupted\\non_menu\\ytJ4lihJrvyzMMRG-WwDNw.jpg\n",
      "Corruption log written to: C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/train\\corruption_log.csv\n",
      "Moved corrupted file: C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/test\\non_menu\\gJH0d6Sut4eZDlbV0GCByg.jpg → C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/test\\corrupted\\non_menu\\gJH0d6Sut4eZDlbV0GCByg.jpg\n",
      "Moved corrupted file: C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/test\\non_menu\\IB2ZjqjtS1W_DadQoPPdgg.jpg → C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/test\\corrupted\\non_menu\\IB2ZjqjtS1W_DadQoPPdgg.jpg\n",
      "Moved corrupted file: C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/test\\non_menu\\iX-8Xm2G7meRHUg8qhoL1A.jpg → C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/test\\corrupted\\non_menu\\iX-8Xm2G7meRHUg8qhoL1A.jpg\n",
      "Moved corrupted file: C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/test\\non_menu\\K6pfRNwGodm1m1gFVQlj-Q.jpg → C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/test\\corrupted\\non_menu\\K6pfRNwGodm1m1gFVQlj-Q.jpg\n",
      "Moved corrupted file: C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/test\\non_menu\\MZj64XNUN6Og178-6XYR6g.jpg → C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/test\\corrupted\\non_menu\\MZj64XNUN6Og178-6XYR6g.jpg\n",
      "Corruption log written to: C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/test\\corruption_log.csv\n"
     ]
    }
   ],
   "source": [
    "# Sanitize training and test folders\n",
    "segregate_corrupted_images(train_dir)\n",
    "segregate_corrupted_images(test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767c62fe",
   "metadata": {},
   "source": [
    "#### 2.2 - Load and preprocess your images\n",
    "\n",
    "Here we prepare the three datasets: **Train, Validation, Test**\n",
    "In doing so, we apply a **resize** of the images to 224x224 pixel.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0647e280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 51402 images belonging to 2 classes.\n",
      "Found 12849 images belonging to 2 classes.\n",
      "Found 9217 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# 1. Set the dimensions and batch size\n",
    "img_height, img_width = 224, 224  # We will resize all images to 224×224\n",
    "batch_size = 32                   # We load 32 images per batch\n",
    "\n",
    "# 2. Prepare an ImageDataGenerator with rescaling and validation split\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,       # Normalize pixel values to [0,1]\n",
    "    validation_split=0.2  # Reserve 20% of images for validation\n",
    ")\n",
    "\n",
    "# 3. Create the training data generator, explicitly using only 'menu' and 'non_menu'\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,                           # Root folder containing class subfolders\n",
    "    target_size=(img_height, img_width), # Resize images to 224×224\n",
    "    batch_size=batch_size,               # 32 images per batch\n",
    "    class_mode='binary',                 # Binary classification\n",
    "    subset='training',                   # 80% of data for training\n",
    "    shuffle=True,                        # Shuffle each epoch\n",
    "    classes=['menu', 'non_menu']         # Only load these two folders\n",
    ")\n",
    "\n",
    "# 4. Create the validation data generator, matching the same class filter\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,                           # Same root as training\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    subset='validation',                 # 20% of data for validation\n",
    "    shuffle=True,\n",
    "    classes=['menu', 'non_menu']         # Exclude 'corrupted'\n",
    ")\n",
    "\n",
    "# 5. Prepare a test data generator (rescaling only)\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1./255       # Normalize test images to [0,1]\n",
    ")\n",
    "\n",
    "# 6. Load test images in order, again limiting to the two valid classes\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,                            # Test folder containing subfolders\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    shuffle=False,                       # Preserve order for evaluation\n",
    "    classes=['menu', 'non_menu']         # Prevent Keras from picking up 'corrupted'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38dc28f",
   "metadata": {},
   "source": [
    "### 3 - Building and Training Different Models\n",
    "Summary: Define and train a simple feedforward neural network to classify images as \"menu\" or \"non_menu\" using a fully connected architecture.\n",
    " 1. Construct the model architecture.\n",
    " 2. Compile the model with appropriate loss function and optimizer.\n",
    " 3. Train the model using training and validation data generators.\n",
    "\n",
    "\n",
    "Beyond accuracy, the following metrics give deeper insight into model performance on imbalanced or critical classes:\n",
    "\n",
    "**Precision** (TP / (TP + FP))\n",
    "Measures how many of the samples predicted as “menu” are actually menus. Useful when false positives are costly (e.g., mis-classifying a non-menu as a menu). It is our case if we use the menus to extract data from it.\n",
    "\n",
    "**Recall** (TP / (TP + FN))\n",
    "Measures how many of the actual “menu” images your model correctly finds. Critical when missing a menu (false negative) is worse than a false alarm.\n",
    "\n",
    "**F1-Score**\n",
    "The harmonic mean of precision\n",
    "​\n",
    "Balances precision vs. recall into a single number.\n",
    "\n",
    "**AUC** (Area Under ROC Curve)\n",
    "Plots the true positive rate vs. false positive rate at various thresholds. AUC close to 1 indicates strong separability, regardless of any specific threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2275255",
   "metadata": {},
   "source": [
    "##### 3.1 - Model 1: **Simple Neural Network**\n",
    "\n",
    "**Model Explanation**: This tiny network flattens images to raw pixel vectors, learns a single hidden representation of size 128, and outputs a single probability. It’s extremely fast but likely underpowered for complex visual patterns.\n",
    "\n",
    "**Pros**:\n",
    "\n",
    " - Extremely fast to train, minimal parameters.\n",
    "\n",
    "- Serves as a baseline to measure benefit of more complex models.\n",
    "\n",
    "**Cons**:\n",
    "\n",
    "- Operates on raw pixels without convolutional feature extraction—struggles on visual tasks.\n",
    "\n",
    "- Poor generalization on complex image patterns.\n",
    "\n",
    "**When to use**:\n",
    "\n",
    "- Quick sanity check or educational demonstration.\n",
    "\n",
    "- As a benchmark before adding convolutional layers or transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48276f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1607/1607 [==============================] - 457s 283ms/step - loss: 0.2930 - accuracy: 0.9752 - precision: 0.9752 - recall: 1.0000 - auc: 0.5240 - val_loss: 0.1340 - val_accuracy: 0.9753 - val_precision: 0.9753 - val_recall: 1.0000 - val_auc: 0.5187\n",
      "Epoch 2/5\n",
      "1607/1607 [==============================] - 457s 284ms/step - loss: 0.1278 - accuracy: 0.9752 - precision: 0.9752 - recall: 1.0000 - auc: 0.6084 - val_loss: 0.1267 - val_accuracy: 0.9753 - val_precision: 0.9753 - val_recall: 1.0000 - val_auc: 0.6030\n",
      "Epoch 3/5\n",
      "1607/1607 [==============================] - 451s 280ms/step - loss: 0.1205 - accuracy: 0.9752 - precision: 0.9752 - recall: 1.0000 - auc: 0.6660 - val_loss: 0.1335 - val_accuracy: 0.9753 - val_precision: 0.9753 - val_recall: 1.0000 - val_auc: 0.5956\n",
      "Epoch 4/5\n",
      "1607/1607 [==============================] - 456s 284ms/step - loss: 0.1133 - accuracy: 0.9752 - precision: 0.9752 - recall: 1.0000 - auc: 0.7146 - val_loss: 0.1149 - val_accuracy: 0.9753 - val_precision: 0.9753 - val_recall: 1.0000 - val_auc: 0.6853\n",
      "Epoch 5/5\n",
      " 220/1607 [===>..........................] - ETA: 6:08 - loss: 0.1035 - accuracy: 0.9757 - precision: 0.9757 - recall: 1.0000 - auc: 0.7770"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 27\u001b[0m\n\u001b[0;32m     14\u001b[0m simple_model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m     15\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# Appropriate for 2-class problems\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,            \u001b[38;5;66;03m# Adam optimizer adapts learning rate automatically\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m     ]\n\u001b[0;32m     24\u001b[0m )\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Train the model for 5 epochs using our generators\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m history_simple \u001b[38;5;241m=\u001b[39m \u001b[43msimple_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;66;43;03m# Batches of 32 training images\u001b[39;49;00m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                    \u001b[49m\u001b[38;5;66;43;03m# Number of full passes over the data\u001b[39;49;00m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_generator\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Evaluate on validation split each epoch\u001b[39;49;00m\n\u001b[0;32m     31\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# loss ---> Lower is better; it measures how “wrong” the model’s predictions are. \u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# accuracy ---> Higher is better; it measures the percentage of correct predictions.\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# val_loss---> Lower is better; it measures how “wrong” the model’s predictions are on the validation set.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# val_accuracy---> Higher is better; it measures the percentage of correct predictions on the validation set.\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03mEpoch 1/5\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;124;03m1607/1607 [==============================] - 458s 284ms/step - loss: 2.5446 - accuracy: 0.9469 - val_loss: 2.5645 - val_accuracy: 0.9753\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;124;03m1607/1607 [==============================] - 429s 267ms/step - loss: 0.1150 - accuracy: 0.9752 - val_loss: 0.1202 - val_accuracy: 0.9753\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1805\u001b[0m ):\n\u001b[0;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1325\u001b[0m     args,\n\u001b[0;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1327\u001b[0m     executing_eagerly)\n\u001b[0;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1501\u001b[0m   )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Build a simple feedforward network for baseline performance\n",
    "simple_model = Sequential([\n",
    "    # Convert 224×224×3 image tensor into a flat 1D vector\n",
    "    Flatten(input_shape=(img_height, img_width, 3)),  \n",
    "    # Fully connected hidden layer with 128 units and ReLU activation\n",
    "    Dense(128, activation='relu'),                    \n",
    "    # Output layer producing a probability via sigmoid (binary classification)\n",
    "    Dense(1, activation='sigmoid')                    \n",
    "])\n",
    "\n",
    "# Configure the learning process\n",
    "simple_model.compile(\n",
    "    loss='binary_crossentropy',  # Appropriate for 2-class problems\n",
    "    optimizer='adam',            # Adam optimizer adapts learning rate automatically\n",
    "    metrics=\n",
    "    \n",
    "    ['accuracy',                     # Overall correctness\n",
    "        Precision(name='precision'),   # Precision metric\n",
    "        Recall(name='recall'),         # Recall metric\n",
    "        AUC(name='auc')                # Area Under ROC\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Train the model for 5 epochs using our generators\n",
    "history_simple = simple_model.fit(\n",
    "    train_generator,             # Batches of 32 training images\n",
    "    epochs=5,                    # Number of full passes over the data\n",
    "    validation_data=validation_generator  # Evaluate on validation split each epoch\n",
    ")\n",
    "\n",
    "# loss ---> Lower is better; it measures how “wrong” the model’s predictions are. \n",
    "# accuracy ---> Higher is better; it measures the percentage of correct predictions.\n",
    "# val_loss---> Lower is better; it measures how “wrong” the model’s predictions are on the validation set.\n",
    "# val_accuracy---> Higher is better; it measures the percentage of correct predictions on the validation set.\n",
    "\n",
    "'''\n",
    "Epoch 1/5\n",
    "1607/1607 [==============================] - 458s 284ms/step - loss: 2.5446 - accuracy: 0.9469 - val_loss: 2.5645 - val_accuracy: 0.9753\n",
    "Epoch 2/5\n",
    "1607/1607 [==============================] - 455s 283ms/step - loss: 0.4110 - accuracy: 0.9563 - val_loss: 0.1470 - val_accuracy: 0.9677\n",
    "Epoch 3/5\n",
    "1607/1607 [==============================] - 466s 290ms/step - loss: 0.1849 - accuracy: 0.9678 - val_loss: 0.2720 - val_accuracy: 0.9753\n",
    "Epoch 4/5\n",
    "1607/1607 [==============================] - 458s 285ms/step - loss: 0.1219 - accuracy: 0.9750 - val_loss: 0.1162 - val_accuracy: 0.9753\n",
    "Epoch 5/5\n",
    "1607/1607 [==============================] - 429s 267ms/step - loss: 0.1150 - accuracy: 0.9752 - val_loss: 0.1202 - val_accuracy: 0.9753\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b479546",
   "metadata": {},
   "source": [
    "#### 3.2 - **VGG16 Transfer Learning**\n",
    "\n",
    "**Model Explanation**: *VGG16*’s deep convolutional filters capture rich visual features. By freezing them and training only the top layers, you leverage powerful pretrained representations without overfitting on limited data.\n",
    "\n",
    "**Pros**:\n",
    "\n",
    "- Leverages deep, hierarchical features learned from millions of images.\n",
    "\n",
    "- Often achieves high accuracy with limited new data.\n",
    "\n",
    "**Cons**:\n",
    "\n",
    "- Large model (~138 million parameters) leads to slower inference and higher memory use.\n",
    "\n",
    "- Freezing all layers may under-utilize domain-specific patterns in your data.\n",
    "\n",
    "**When to use**:\n",
    "\n",
    "- When you have limited labeled images but need strong feature extractors.\n",
    "\n",
    "- For desktop/server training where inference speed and size are less critical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb41c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VGG16 base pretrained on ImageNet, excluding its top dense layers\n",
    "vgg_base = VGG16(\n",
    "    weights='imagenet',        # Load pretrained weights\n",
    "    include_top=False,         # Drop final classification block\n",
    "    input_shape=(img_height, img_width, 3)  # Match our input dimensions\n",
    ")\n",
    "# Prevent updates to the VGG16 layers during training\n",
    "vgg_base.trainable = False  \n",
    "\n",
    "# Stack custom classification layers on top of VGG16\n",
    "vgg_model = Sequential([\n",
    "    vgg_base,                  # Pretrained convolutional feature extractor\n",
    "    Flatten(),                 # Flatten feature maps to 1D\n",
    "    Dense(256, activation='relu'),  # Learn higher-level combinations\n",
    "    Dense(1, activation='sigmoid')  # Binary classification output\n",
    "])\n",
    "\n",
    "# Compile with a low learning rate for stable fine-tuning\n",
    "vgg_model.compile(\n",
    "    loss='binary_crossentropy',  \n",
    "    optimizer=Adam(learning_rate=1e-4),  # Gentle weight updates\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train for 5 epochs, evaluating on the validation set\n",
    "history_vgg = vgg_model.fit(\n",
    "    train_generator,\n",
    "    epochs=5,\n",
    "    validation_data=validation_generator\n",
    ")\n",
    "\n",
    "\n",
    "'''\n",
    "Epoch 1/5\n",
    " 353/1607 [=====>........................] - ETA: 52:56 - loss: 0.0444 - accuracy: 0.9874\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70ebbb2",
   "metadata": {},
   "source": [
    "#### 3.3 - **MobileNetV2** Transfer Learning\n",
    "\n",
    "**Model Explanation**: *MobileNetV2* offers a much smaller footprint than VGG16, making it ideal for faster training and mobile deployment. Its inverted residual blocks preserve accuracy while reducing parameters.\n",
    "\n",
    "**Pros**:\n",
    "\n",
    "- Much smaller and faster than VGG16, suitable for edge or mobile deployment.\n",
    "\n",
    "- Maintains competitive accuracy through efficient inverted residual blocks.\n",
    "\n",
    "**Cons**:\n",
    "\n",
    "- May underperform compared to deeper models on very complex image tasks.\n",
    "\n",
    "- Limited capacity for domain-specific fine-tuning when frozen.\n",
    "\n",
    "**When to use**:\n",
    "\n",
    "- When inference speed and model size are critical.\n",
    "\n",
    "- For deployment on devices with constrained compute or memory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c338b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1607/1607 [==============================] - 808s 501ms/step - loss: 0.0192 - accuracy: 0.9951 - val_loss: 0.0141 - val_accuracy: 0.9957\n",
      "Epoch 2/5\n",
      "1607/1607 [==============================] - 802s 499ms/step - loss: 0.0047 - accuracy: 0.9983 - val_loss: 0.0351 - val_accuracy: 0.9933\n",
      "Epoch 3/5\n",
      "1607/1607 [==============================] - 803s 500ms/step - loss: 0.0016 - accuracy: 0.9993 - val_loss: 0.0199 - val_accuracy: 0.9957\n",
      "Epoch 4/5\n",
      "1607/1607 [==============================] - 802s 499ms/step - loss: 6.3588e-04 - accuracy: 0.9998 - val_loss: 0.0212 - val_accuracy: 0.9955\n",
      "Epoch 5/5\n",
      "1607/1607 [==============================] - 804s 500ms/step - loss: 8.6918e-04 - accuracy: 0.9997 - val_loss: 0.0279 - val_accuracy: 0.9951\n"
     ]
    }
   ],
   "source": [
    "# Load MobileNetV2 base pretrained on ImageNet without its top layers\n",
    "mobilenet_base = MobileNetV2(\n",
    "    weights='imagenet',        # Pretrained weights\n",
    "    include_top=False,         # Exclude original classifier\n",
    "    input_shape=(img_height, img_width, 3)\n",
    ")\n",
    "# Freeze the convolutional base to retain pretrained features\n",
    "mobilenet_base.trainable = False  \n",
    "\n",
    "# Build a lightweight classifier on top\n",
    "mobilenet_model = Sequential([\n",
    "    mobilenet_base,            # Mobile-friendly feature extractor\n",
    "    Flatten(),                 # Flatten feature maps\n",
    "    Dense(128, activation='relu'),  # Compact dense layer\n",
    "    Dense(1, activation='sigmoid')  # Binary output\n",
    "])\n",
    "\n",
    "# Compile with small learning rate to refine top layers\n",
    "mobilenet_model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=Adam(learning_rate=1e-4),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train for 5 epochs, validating each epoch\n",
    "history_mobile = mobilenet_model.fit(\n",
    "    train_generator,\n",
    "    epochs=5,\n",
    "    validation_data=validation_generator\n",
    ")\n",
    "\n",
    "\n",
    "'''Epoch 1/5\n",
    "1607/1607 [==============================] - 808s 501ms/step - loss: 0.0192 - accuracy: 0.9951 - val_loss: 0.0141 - val_accuracy: 0.9957\n",
    "Epoch 2/5\n",
    "1607/1607 [==============================] - 802s 499ms/step - loss: 0.0047 - accuracy: 0.9983 - val_loss: 0.0351 - val_accuracy: 0.9933\n",
    "Epoch 3/5\n",
    "1607/1607 [==============================] - 803s 500ms/step - loss: 0.0016 - accuracy: 0.9993 - val_loss: 0.0199 - val_accuracy: 0.9957\n",
    "Epoch 4/5\n",
    "1607/1607 [==============================] - 802s 499ms/step - loss: 6.3588e-04 - accuracy: 0.9998 - val_loss: 0.0212 - val_accuracy: 0.9955\n",
    "Epoch 5/5\n",
    "1607/1607 [==============================] - 804s 500ms/step - loss: 8.6918e-04 - accuracy: 0.9997 - val_loss: 0.0279 - val_accuracy: 0.9951'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429f5615",
   "metadata": {},
   "source": [
    "### 4 - Test on the held-out test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a700911b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports for metrics, plotting, and filesystem operations\n",
    "import os\n",
    "from sklearn.metrics import classification_report, confusion_matrix  # Evaluation tools\n",
    "import matplotlib.pyplot as plt                                     # Plotting\n",
    "import seaborn as sns                                              # Heatmap\n",
    "# Note: ensure seaborn and matplotlib are imported in your notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8d1f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Point to the model you’ve chosen (e.g., vgg_model, simple_model, or mobilenet_model)\n",
    "chosen_model = vgg_model  # replace with your selected model variable\n",
    "\n",
    "# 3. Evaluate on the test set: returns [loss, accuracy, precision, recall, auc]\n",
    "test_metrics = chosen_model.evaluate(test_generator)\n",
    "# Unpack for readability\n",
    "test_loss, test_accuracy, test_precision, test_recall, test_auc = test_metrics\n",
    "\n",
    "# 4. Print the test metrics\n",
    "print(f\"Test Loss: {test_loss:.4f}\")         \n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test Precision: {test_precision:.4f}\")\n",
    "print(f\"Test Recall: {test_recall:.4f}\")\n",
    "print(f\"Test AUC: {test_auc:.4f}\")\n",
    "\n",
    "# 5. Generate predictions on the test set\n",
    "probabilities = chosen_model.predict(test_generator)        # Model outputs probabilities\n",
    "pred_labels = (probabilities > 0.5).astype(int).ravel()     # Convert to 0/1 labels\n",
    "\n",
    "# 6. Print a detailed classification report\n",
    "print(classification_report(\n",
    "    test_generator.classes,    # True labels\n",
    "    pred_labels,               # Predicted labels\n",
    "    target_names=['non_menu','menu']\n",
    "))\n",
    "\n",
    "# 7. Plot the confusion matrix\n",
    "cm = confusion_matrix(test_generator.classes, pred_labels)  \n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=['non_menu','menu'],\n",
    "    yticklabels=['non_menu','menu']\n",
    ")\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix on Test Set')\n",
    "plt.show()\n",
    "\n",
    "# 8. Ensure the save directory exists\n",
    "save_dir = r\"C:\\Users\\leomo\\Desktop\\DATA SCIENTIST CAREER\\PROJECTS\\Menu_Classifier\\saved_models\"\n",
    "os.makedirs(save_dir, exist_ok=True)  # Create folder if it doesn’t exist\n",
    "\n",
    "# 9. Save the trained model for later use\n",
    "save_path = os.path.join(save_dir, \"menu_classifier.h5\")\n",
    "chosen_model.save(save_path)  \n",
    "print(f\"Model saved to: {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
