{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "690a7e01",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Menu Classifier\n",
    "---\n",
    "---\n",
    "\n",
    "This notebook shows the ML model to detect Menu pictures\n",
    "\n",
    "\n",
    "**a) Data from Yelp**:\n",
    "We use a sample of aproximately 65000+ images of yelp dataset. \n",
    "This dataset contains over 65,000 images from Yelp, categorized into five classes: Food, Inside, Outside, Drink, and Menu. The dataset is split into training and test sets, with the training set comprising 95% of the images and the test set comprising 5% of the images. The metadata includes photo IDs, business IDs, captions, and labels for each image.\n",
    "\n",
    "**b) Dataset Structure**:\n",
    "\n",
    "The metadata is provided in a CSV file with the following columns:\n",
    "\n",
    "**photo_id**: Unique identifier for each photo.\\\n",
    "**business_id**: Unique identifier for the business associated with the photo.\\\n",
    "**caption**: Description or caption associated with the photo.\\\n",
    "**label**: Class label of the photo (Food, Inside, Outside, Drink, Menu).\\\n",
    "\n",
    "**c) Images**\n",
    "The images are stored in separate directories for the training and test sets. Each directory contains subdirectories for each class label."
   ]
  },
  {
   "cell_type": "raw",
   "id": "23aba664",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Menu_Classifier/\n",
    "├── data_processed/\n",
    "│   ├── train/\n",
    "│   │   ├── menu/\n",
    "│   │   └── non_menu/\n",
    "│   └── test/\n",
    "│       ├── menu/\n",
    "│       └── non_menu/\n",
    "├── notebooks/\n",
    "│   └── menu_classifier.ipynb\n",
    "├── src/\n",
    "│   └── utils.py\n",
    "├── requirements.txt\n",
    "└── README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652e3be7",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### 1 - Initial Setup and Imports\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "654311e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import os\n",
    "import csv  \n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt                                     # Plotting  \n",
    "import seaborn as sns                                              # Heatmap\n",
    "\n",
    "# Image processing\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.applications import VGG16, MobileNetV2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Define paths\n",
    "train_dir = \"C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/train\"\n",
    "test_dir = \"C:/Users/leomo/Desktop/DATA SCIENTIST CAREER/PROJECTS/Menu_Classifier/data_processed/test\"\n",
    "\n",
    "# Only these extensions will be treated as images\n",
    "image_extensions = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2534155",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### 2 - Data Loading and Preprocessing\n",
    "---\n",
    "---\n",
    "We use Keras ImageDataGenerator to load and preprocess images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767cbaeb",
   "metadata": {},
   "source": [
    "---\n",
    "#### 2.1 - Clean out corrupted/non-image files\n",
    "---\n",
    "Keras’ flow_from_directory will try to open every file in your menu/non_menu folders. If it encounters a file that isn’t a valid JPEG/PNG (e.g. a corrupted image, or a stray system file like Thumbs.db), PIL raises UnidentifiedImageError."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0438650c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the Reusable Cleanup Function\n",
    "def segregate_corrupted_images(\n",
    "    root_dir,\n",
    "    corrupt_dir_name=\"corrupted\",\n",
    "    log_filename=\"corruption_log.csv\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Moves unreadable or non-image files into a 'corrupted' folder and logs each move with:\n",
    "      - original path\n",
    "      - new path\n",
    "      - file size (bytes)\n",
    "      - last-modified timestamp (ISO)\n",
    "    Q1: Log extended with size & timestamp.\n",
    "    Q3: Encapsulated for reuse across projects.\n",
    "    \"\"\"\n",
    "    # Build paths\n",
    "    corrupt_dir = os.path.join(root_dir, corrupt_dir_name)  \n",
    "    os.makedirs(corrupt_dir, exist_ok=True)  # Create if missing\n",
    "    log_path = os.path.join(root_dir, log_filename)  \n",
    "\n",
    "    # Open CSV and write header\n",
    "    with open(log_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as log_file:\n",
    "        writer = csv.writer(log_file)  \n",
    "        writer.writerow([\n",
    "            \"original_path\",\n",
    "            \"moved_path\",\n",
    "            \"file_size_bytes\",\n",
    "            \"last_modified\"\n",
    "        ])\n",
    "\n",
    "        # Walk through all files under root_dir\n",
    "        for subdir, _, files in os.walk(root_dir):\n",
    "            # Skip any files already in the corrupted folder\n",
    "            if corrupt_dir in subdir:\n",
    "                continue\n",
    "\n",
    "            for fname in files:\n",
    "                # Skip the log file itself\n",
    "                if fname == log_filename:\n",
    "                    continue\n",
    "                # Only attempt images with allowed extensions\n",
    "                if not fname.lower().endswith(image_extensions):\n",
    "                    continue\n",
    "\n",
    "                file_path = os.path.join(subdir, fname)  # Full file path\n",
    "                try:\n",
    "                    # Attempt to open and verify image integrity\n",
    "                    img = Image.open(file_path)  \n",
    "                    img.verify()  \n",
    "                except Exception:\n",
    "                    # On failure, gather diagnostics\n",
    "                    file_size = os.path.getsize(file_path)  \n",
    "                    mtime = os.path.getmtime(file_path)  \n",
    "                    last_mod = datetime.fromtimestamp(mtime).isoformat()  \n",
    "\n",
    "                    # Determine destination and ensure its folder exists\n",
    "                    rel_path = os.path.relpath(file_path, root_dir)\n",
    "                    dest_path = os.path.join(corrupt_dir, rel_path)\n",
    "                    os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n",
    "\n",
    "                    # Move the bad file and log its details\n",
    "                    shutil.move(file_path, dest_path)  \n",
    "                    writer.writerow([\n",
    "                        file_path,\n",
    "                        dest_path,\n",
    "                        file_size,\n",
    "                        last_mod\n",
    "                    ])\n",
    "                    print(f\"Moved corrupted file: {file_path} → {dest_path}\")\n",
    "\n",
    "    print(f\"Corruption log written to: {log_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c65bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanitize training and test folders\n",
    "segregate_corrupted_images(train_dir)\n",
    "segregate_corrupted_images(test_dir)\n",
    "\n",
    "# 35 minutes to run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767c62fe",
   "metadata": {},
   "source": [
    "---\n",
    "#### 2.2 - Load and preprocess your images\n",
    "---\n",
    "Here we prepare the three datasets: **Train, Validation, Test**\n",
    "In doing so, we apply a **resize** of the images to 224x224 pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0647e280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 51397 images belonging to 2 classes.\n",
      "Found 12848 images belonging to 2 classes.\n",
      "Found 9217 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# 1. Set the dimensions and batch size\n",
    "img_height, img_width = 224, 224  # We will resize all images to 224×224\n",
    "batch_size = 32                   # We load 32 images per batch\n",
    "\n",
    "# 2. Prepare an ImageDataGenerator with rescaling and validation split\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,       # Normalize pixel values to [0,1]\n",
    "    validation_split=0.2  # Reserve 20% of images for validation\n",
    ")\n",
    "\n",
    "# 3. Create the training data generator, explicitly using only 'menu' and 'non_menu'\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,                           # Root folder containing class subfolders\n",
    "    target_size=(img_height, img_width), # Resize images to 224×224\n",
    "    batch_size=batch_size,               # 32 images per batch\n",
    "    class_mode='binary',                 # Binary classification\n",
    "    subset='training',                   # 80% of data for training\n",
    "    shuffle=True,                        # Shuffle each epoch\n",
    "    classes=['menu', 'non_menu']         # Only load these two folders\n",
    ")\n",
    "\n",
    "# 4. Create the validation data generator, matching the same class filter\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,                           # Same root as training\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    subset='validation',                 # 20% of data for validation\n",
    "    shuffle=True,\n",
    "    classes=['menu', 'non_menu']         # Exclude 'corrupted'\n",
    ")\n",
    "\n",
    "# 5. Prepare a test data generator (rescaling only)\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1./255       # Normalize test images to [0,1]\n",
    ")\n",
    "\n",
    "# 6. Load test images in order, again limiting to the two valid classes\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,                            # Test folder containing subfolders\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    shuffle=False,                       # Preserve order for evaluation\n",
    "    classes=['menu', 'non_menu']         # Prevent Keras from picking up 'corrupted'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2928df",
   "metadata": {},
   "source": [
    "---\n",
    "#### 2.3 - Inspect the class-to-index mapping\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaefa00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train generator class indices: {'menu': 0, 'non_menu': 1}\n",
      "Validation generator class indices: {'menu': 0, 'non_menu': 1}\n",
      "Test generator class indices: {'menu': 0, 'non_menu': 1}\n"
     ]
    }
   ],
   "source": [
    "# Print mapping for the training set\n",
    "print(\"Train generator class indices:\", train_generator.class_indices)\n",
    "# e.g., {'menu': 0, 'non_menu': 1}\n",
    "\n",
    "# Print mapping for the validation set\n",
    "print(\"Validation generator class indices:\", validation_generator.class_indices)\n",
    "# Should match the training mapping\n",
    "\n",
    "# Print mapping for the test set\n",
    "print(\"Test generator class indices:\", test_generator.class_indices)\n",
    "# Should also match\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38dc28f",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### 3 - Building and Training Different Models\n",
    "---\n",
    "---\n",
    "Summary: Define and train a simple feedforward neural network to classify images as \"menu\" or \"non_menu\" using a fully connected architecture.\n",
    " 1. Construct the model architecture.\n",
    " 2. Compile the model with appropriate loss function and optimizer.\n",
    " 3. Train the model using training and validation data generators.\n",
    "\n",
    "\n",
    "Beyond accuracy, the following metrics give deeper insight into model performance on imbalanced or critical classes:\n",
    "\n",
    "**Precision** (TP / (TP + FP))\n",
    "Measures how many of the samples predicted as “menu” are actually menus. Useful when false positives are costly (e.g., mis-classifying a non-menu as a menu). It is our case if we use the menus to extract data from it.\n",
    "\n",
    "**Recall** (TP / (TP + FN))\n",
    "Measures how many of the actual “menu” images your model correctly finds. Critical when missing a menu (false negative) is worse than a false alarm.\n",
    "\n",
    "**F1-Score**\n",
    "The harmonic mean of precision\n",
    "​\n",
    "Balances precision vs. recall into a single number.\n",
    "\n",
    "**AUC** (Area Under ROC Curve)\n",
    "Plots the true positive rate vs. false positive rate at various thresholds. AUC close to 1 indicates strong separability, regardless of any specific threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2275255",
   "metadata": {},
   "source": [
    "---\n",
    "##### 3.1 - Model 1: **Simple Neural Network**\n",
    "---\n",
    "\n",
    "\n",
    "**Model Explanation**: This tiny network flattens images to raw pixel vectors, learns a single hidden representation of size 128, and outputs a single probability. It’s extremely fast but likely underpowered for complex visual patterns.\n",
    "\n",
    "**Pros**:\n",
    "\n",
    " - Extremely fast to train, minimal parameters.\n",
    "\n",
    "- Serves as a baseline to measure benefit of more complex models.\n",
    "\n",
    "**Cons**:\n",
    "\n",
    "- Operates on raw pixels without convolutional feature extraction—struggles on visual tasks.\n",
    "\n",
    "- Poor generalization on complex image patterns.\n",
    "\n",
    "**When to use**:\n",
    "\n",
    "- Quick sanity check or educational demonstration.\n",
    "\n",
    "- As a benchmark before adding convolutional layers or transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48276f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple feedforward network for baseline performance\n",
    "simple_model = Sequential([\n",
    "    # Convert 224×224×3 image tensor into a flat 1D vector\n",
    "    Flatten(input_shape=(img_height, img_width, 3)),  \n",
    "    # Fully connected hidden layer with 128 units and ReLU activation\n",
    "    Dense(128, activation='relu'),                    \n",
    "    # Output layer producing a probability via sigmoid (binary classification)\n",
    "    Dense(1, activation='sigmoid')                    \n",
    "])\n",
    "\n",
    "# Configure the learning process\n",
    "simple_model.compile(\n",
    "    loss='binary_crossentropy',  # Appropriate for 2-class problems\n",
    "    optimizer='adam',            # Adam optimizer adapts learning rate automatically\n",
    "    metrics=['accuracy',                     # Overall correctness\n",
    "        Precision(name='precision'),   # Precision metric\n",
    "        Recall(name='recall'),         # Recall metric\n",
    "        AUC(name='auc')                # Area Under ROC\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3194d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the model for 5 epochs using our generators\n",
    "history_simple = simple_model.fit(\n",
    "    train_generator,             # Batches of 32 training images\n",
    "    epochs=5,                    # Number of full passes over the data\n",
    "    validation_data=validation_generator  # Evaluate on validation split each epoch\n",
    ")\n",
    "\n",
    "# loss ---> Lower is better; it measures how “wrong” the model’s predictions are. \n",
    "# accuracy ---> Higher is better; it measures the percentage of correct predictions.\n",
    "# val_loss---> Lower is better; it measures how “wrong” the model’s predictions are on the validation set.\n",
    "# val_accuracy---> Higher is better; it measures the percentage of correct predictions on the validation set.\n",
    "\n",
    "'''\n",
    "Epoch 1/5\n",
    "1607/1607 [==============================] - 458s 284ms/step - loss: 2.5446 - accuracy: 0.9469 - val_loss: 2.5645 - val_accuracy: 0.9753\n",
    "Epoch 2/5\n",
    "1607/1607 [==============================] - 455s 283ms/step - loss: 0.4110 - accuracy: 0.9563 - val_loss: 0.1470 - val_accuracy: 0.9677\n",
    "Epoch 3/5\n",
    "1607/1607 [==============================] - 466s 290ms/step - loss: 0.1849 - accuracy: 0.9678 - val_loss: 0.2720 - val_accuracy: 0.9753\n",
    "Epoch 4/5\n",
    "1607/1607 [==============================] - 458s 285ms/step - loss: 0.1219 - accuracy: 0.9750 - val_loss: 0.1162 - val_accuracy: 0.9753\n",
    "Epoch 5/5\n",
    "1607/1607 [==============================] - 429s 267ms/step - loss: 0.1150 - accuracy: 0.9752 - val_loss: 0.1202 - val_accuracy: 0.9753\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b479546",
   "metadata": {},
   "source": [
    "---\n",
    "#### 3.2 - **VGG16 Transfer Learning**\n",
    "---\n",
    "**Model Explanation**: *VGG16*’s deep convolutional filters capture rich visual features. By freezing them and training only the top layers, you leverage powerful pretrained representations without overfitting on limited data.\n",
    "\n",
    "**Pros**:\n",
    "\n",
    "- Leverages deep, hierarchical features learned from millions of images.\n",
    "\n",
    "- Often achieves high accuracy with limited new data.\n",
    "\n",
    "**Cons**:\n",
    "\n",
    "- Large model (~138 million parameters) leads to slower inference and higher memory use.\n",
    "\n",
    "- Freezing all layers may under-utilize domain-specific patterns in your data.\n",
    "\n",
    "**When to use**:\n",
    "\n",
    "- When you have limited labeled images but need strong feature extractors.\n",
    "\n",
    "- For desktop/server training where inference speed and size are less critical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb41c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VGG16 base pretrained on ImageNet, excluding its top dense layers\n",
    "vgg_base = VGG16(\n",
    "    weights='imagenet',        # Load pretrained weights\n",
    "    include_top=False,         # Drop final classification block\n",
    "    input_shape=(img_height, img_width, 3)  # Match our input dimensions\n",
    ")\n",
    "# Prevent updates to the VGG16 layers during training\n",
    "vgg_base.trainable = False  \n",
    "\n",
    "# Stack custom classification layers on top of VGG16\n",
    "vgg_model = Sequential([\n",
    "    vgg_base,                  # Pretrained convolutional feature extractor\n",
    "    Flatten(),                 # Flatten feature maps to 1D\n",
    "    Dense(256, activation='relu'),  # Learn higher-level combinations\n",
    "    Dense(1, activation='sigmoid')  # Binary classification output\n",
    "])\n",
    "\n",
    "# Compile with a low learning rate for stable fine-tuning\n",
    "vgg_model.compile(\n",
    "    loss='binary_crossentropy',  \n",
    "    optimizer=Adam(learning_rate=1e-4),  # Gentle weight updates\n",
    "    metrics=['accuracy',                     # Overall correctness\n",
    "        Precision(name='precision'),   # Precision metric\n",
    "        Recall(name='recall'),         # Recall metric\n",
    "        AUC(name='auc')                # Area Under ROC\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0352b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train for 5 epochs, evaluating on the validation set\n",
    "history_vgg = vgg_model.fit(\n",
    "    train_generator,\n",
    "    epochs=5,\n",
    "    validation_data=validation_generator\n",
    ")\n",
    "'''\n",
    "Epoch 1/5\n",
    " 353/1607 [=====>........................] - ETA: 52:56 - loss: 0.0444 - accuracy: 0.9874\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70ebbb2",
   "metadata": {},
   "source": [
    "---\n",
    "#### 3.3 - **MobileNetV2** Transfer Learning\n",
    "---\n",
    "**Model Explanation**: *MobileNetV2* offers a much smaller footprint than VGG16, making it ideal for faster training and mobile deployment. Its inverted residual blocks preserve accuracy while reducing parameters.\n",
    "\n",
    "**Pros**:\n",
    "\n",
    "- Much smaller and faster than VGG16, suitable for edge or mobile deployment.\n",
    "\n",
    "- Maintains competitive accuracy through efficient inverted residual blocks.\n",
    "\n",
    "**Cons**:\n",
    "\n",
    "- May underperform compared to deeper models on very complex image tasks.\n",
    "\n",
    "- Limited capacity for domain-specific fine-tuning when frozen.\n",
    "\n",
    "**When to use**:\n",
    "\n",
    "- When inference speed and model size are critical.\n",
    "\n",
    "- For deployment on devices with constrained compute or memory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11c338b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\leomo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load MobileNetV2 base pretrained on ImageNet without its top layers\n",
    "mobilenet_base = MobileNetV2(\n",
    "    weights='imagenet',        # Pretrained weights\n",
    "    include_top=False,         # Exclude original classifier\n",
    "    input_shape=(img_height, img_width, 3)\n",
    ")\n",
    "# Freeze the convolutional base to retain pretrained features\n",
    "mobilenet_base.trainable = False  \n",
    "\n",
    "# Build a lightweight classifier on top\n",
    "mobilenet_model = Sequential([\n",
    "    mobilenet_base,            # Mobile-friendly feature extractor\n",
    "    Flatten(),                 # Flatten feature maps\n",
    "    Dense(128, activation='relu'),  # Compact dense layer\n",
    "    Dense(1, activation='sigmoid')  # Binary output\n",
    "])\n",
    "\n",
    "# Compile with small learning rate to refine top layers\n",
    "mobilenet_model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=Adam(learning_rate=1e-4),\n",
    "        metrics=['accuracy',                     # Overall correctness\n",
    "        Precision(name='precision'),   # Precision metric\n",
    "        Recall(name='recall'),         # Recall metric\n",
    "        AUC(name='auc')                # Area Under ROC\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9231fb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for 5 epochs, validating each epoch\n",
    "history_mobile = mobilenet_model.fit(\n",
    "    train_generator,\n",
    "    epochs=5,\n",
    "    validation_data=validation_generator\n",
    ")\n",
    "\n",
    "\n",
    "'''Epoch 1/5\n",
    "1607/1607 [==============================] - 808s 501ms/step - loss: 0.0192 - accuracy: 0.9951 - val_loss: 0.0141 - val_accuracy: 0.9957\n",
    "Epoch 2/5\n",
    "1607/1607 [==============================] - 802s 499ms/step - loss: 0.0047 - accuracy: 0.9983 - val_loss: 0.0351 - val_accuracy: 0.9933\n",
    "Epoch 3/5\n",
    "1607/1607 [==============================] - 803s 500ms/step - loss: 0.0016 - accuracy: 0.9993 - val_loss: 0.0199 - val_accuracy: 0.9957\n",
    "Epoch 4/5\n",
    "1607/1607 [==============================] - 802s 499ms/step - loss: 6.3588e-04 - accuracy: 0.9998 - val_loss: 0.0212 - val_accuracy: 0.9955\n",
    "Epoch 5/5\n",
    "1607/1607 [==============================] - 804s 500ms/step - loss: 8.6918e-04 - accuracy: 0.9997 - val_loss: 0.0279 - val_accuracy: 0.9951'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe093d5",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### 4 - Functions for building and testing a model\n",
    "---\n",
    "---\n",
    "Encapsulating model creation and training into reusable functions makes it easy to swap architectures or hyperparameters without copy-pasting code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db417288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble and compile a binary classifier on top of a frozen base_model.\n",
    "def build_classifier(base_model, top_units, learning_rate, fine_tune_layers=0):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      base_model: pretrained Keras Model (e.g. VGG16(include_top=False)).\n",
    "      top_units: int, number of neurons in the dense layer.\n",
    "      learning_rate: float, learning rate for Adam.\n",
    "      fine_tune_layers: int, how many of the top conv layers to unfreeze (0 = freeze all).\n",
    "    Returns:\n",
    "      A compiled Keras Sequential model.\n",
    "    \"\"\"\n",
    "    # 1. Determine how many layers to unfreeze\n",
    "    if fine_tune_layers > 0:\n",
    "        # Unfreeze the last `fine_tune_layers` layers\n",
    "        for layer in base_model.layers[:-fine_tune_layers]:\n",
    "            layer.trainable = False\n",
    "        for layer in base_model.layers[-fine_tune_layers:]:\n",
    "            layer.trainable = True\n",
    "    else:\n",
    "        # Freeze the entire base if fine_tune_layers == 0\n",
    "        base_model.trainable = False\n",
    "\n",
    "    # 2. Stack the classifier head\n",
    "    model = Sequential([\n",
    "        base_model,                          # pretrained feature extractor\n",
    "        Flatten(),                           # flatten to 1D\n",
    "        Dense(top_units, activation='relu'), # dense layer with `top_units` neurons\n",
    "        Dense(1, activation='sigmoid')       # single-unit sigmoid for binary output\n",
    "    ])\n",
    "\n",
    "    # 3. Compile with multiple metrics\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',               # binary crossentropy loss\n",
    "        optimizer=Adam(learning_rate=learning_rate),  # Adam optimizer\n",
    "        metrics=[                                 # metrics to track\n",
    "            'accuracy',\n",
    "            Precision(name='precision'),\n",
    "            Recall(name='recall'),\n",
    "            AUC(name='auc')\n",
    "        ]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_and_evaluate(model, name, train_gen, val_gen, epochs=5):\n",
    "    \"\"\"\n",
    "    Train the model and evaluate on the validation set.\n",
    "    Args:\n",
    "      model: compiled Keras model.\n",
    "      name: string, a label for this model (e.g., 'VGG16').\n",
    "      train_gen: ImageDataGenerator for training.\n",
    "      val_gen: ImageDataGenerator for validation.\n",
    "      epochs: int, number of epochs.\n",
    "    Returns:\n",
    "      history: Training history object.\n",
    "      results: dict mapping metric names to values (plus 'model' key).\n",
    "    \"\"\"\n",
    "    # 1. Fit the model\n",
    "    history = model.fit(\n",
    "        train_gen,              # training data\n",
    "        epochs=epochs,          # number of epochs\n",
    "        validation_data=val_gen # validation data\n",
    "    )\n",
    "\n",
    "    # 2. Evaluate on validation set to get final scores\n",
    "    scores = model.evaluate(val_gen)        # returns list: [loss, acc, prec, rec, auc]\n",
    "    names = model.metrics_names             # e.g. ['loss','accuracy','precision','recall','auc']\n",
    "\n",
    "    # 3. Build a results dict\n",
    "    results = {'model': name}\n",
    "    for m_name, m_val in zip(names, scores):\n",
    "        results[m_name] = m_val             # e.g. results['precision'] = 0.92\n",
    "\n",
    "    return history, results\n",
    "\n",
    "\n",
    "def compare_model_performance(results_list):\n",
    "    \"\"\"\n",
    "    Create, display, and return a DataFrame comparing each model’s metrics.\n",
    "\n",
    "    Args:\n",
    "      results_list: list of dicts returned by train_and_evaluate, each containing:\n",
    "        - 'model': model name\n",
    "        - metric names as keys (e.g., 'loss','accuracy','precision', etc.)\n",
    "\n",
    "    Returns:\n",
    "      df: pandas DataFrame indexed by 'model', with columns sorted (loss first).\n",
    "    \"\"\"\n",
    "    # Build a DataFrame from the list of result dicts\n",
    "    df = pd.DataFrame(results_list)                       # each dict → one row\n",
    "\n",
    "    # Use the 'model' column as the index for readability\n",
    "    df.set_index('model', inplace=True)                   # rows labeled by model name\n",
    "\n",
    "    # Reorder columns: put 'loss' first, then the rest alphabetically\n",
    "    cols = ['loss'] + sorted([c for c in df.columns if c != 'loss'])\n",
    "    df = df[cols]                                         # reorder columns\n",
    "\n",
    "    # Print a markdown-formatted table for quick console viewing\n",
    "    print(df.to_markdown())                               # human-readable table\n",
    "\n",
    "    # Return the DataFrame so you can assign it and use it later\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266de5b6",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### 5 - Testing the Models and Showing the peroformances\n",
    "---\n",
    "---\n",
    "\n",
    "#### 5.1 - Testing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce0323b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1607/1607 [==============================] - 455s 282ms/step - loss: 0.7106 - accuracy: 0.9649 - precision: 0.9758 - recall: 0.9886 - auc: 0.5395 - val_loss: 0.1617 - val_accuracy: 0.9753 - val_precision: 0.9753 - val_recall: 1.0000 - val_auc: 0.4930\n",
      "402/402 [==============================] - 45s 112ms/step - loss: 0.1617 - accuracy: 0.9753 - precision: 0.9753 - recall: 1.0000 - auc: 0.4930\n",
      "1607/1607 [==============================] - 811s 503ms/step - loss: 0.0210 - accuracy: 0.9947 - precision: 0.9969 - recall: 0.9977 - auc: 0.9816 - val_loss: 0.0177 - val_accuracy: 0.9947 - val_precision: 0.9970 - val_recall: 0.9975 - val_auc: 0.9862\n",
      "402/402 [==============================] - 141s 349ms/step - loss: 0.0177 - accuracy: 0.9947 - precision: 0.9970 - recall: 0.9975 - auc: 0.9862\n",
      "| model       |      loss |   accuracy |      auc |   precision |   recall |\n",
      "|:------------|----------:|-----------:|---------:|------------:|---------:|\n",
      "| SimpleNN    | 0.161652  |   0.975327 | 0.493013 |    0.975327 | 1        |\n",
      "| MobileNetV2 | 0.0176959 |   0.994707 | 0.986197 |    0.997049 | 0.997526 |\n"
     ]
    }
   ],
   "source": [
    "# Build and train three models, collecting their results\n",
    "hist_simple, res_simple = train_and_evaluate(\n",
    "    simple_model, 'SimpleNN', train_generator, validation_generator, epochs=1\n",
    ")\n",
    "'''\n",
    "vgg = build_classifier(vgg_base, top_units=256, learning_rate=1e-4)\n",
    "hist_vgg, res_vgg = train_and_evaluate(\n",
    "    vgg, 'VGG16', train_generator, validation_generator\n",
    ")\n",
    "'''\n",
    "\n",
    "mobile = build_classifier(mobilenet_base, top_units=128, learning_rate=1e-4)\n",
    "\n",
    "hist_mobile, res_mobile = train_and_evaluate(\n",
    "    mobile, 'MobileNetV2', train_generator, validation_generator, epochs=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8f544a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| model       |      loss |   accuracy |      auc |   precision |   recall |\n",
      "|:------------|----------:|-----------:|---------:|------------:|---------:|\n",
      "| SimpleNN    | 0.161652  |   0.975327 | 0.493013 |    0.975327 | 1        |\n",
      "| MobileNetV2 | 0.0176959 |   0.994707 | 0.986197 |    0.997049 | 0.997526 |\n"
     ]
    }
   ],
   "source": [
    "# After collecting your results:\n",
    "results = [res_simple, res_mobile]\n",
    "\n",
    "# This call prints the table and gives you the df to work with\n",
    "df_metrics = compare_model_performance(results)\n",
    "\n",
    "# Now you can plot, export, or manipulate df_metrics further:\n",
    "# e.g., df_metrics.to_csv(\"model_comparison.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd9dc1a",
   "metadata": {},
   "source": [
    "##### 5.1.2 - k-Fold Cross-Validation with Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e03a6ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "def cross_validate_models(build_fn, modelspecs, X_generator, y, folds=5):\n",
    "    \"\"\"\n",
    "    Performs stratified k-fold CV for multiple model specs.\n",
    "    \n",
    "    Args:\n",
    "      build_fn: function that takes (spec) → compiled model\n",
    "      modelspecs: list of dicts, each with keys ['name','base_model','top_units','lr']\n",
    "      X_generator: function(indices) → (train_gen, val_gen) given row indices\n",
    "      y: array of true labels for all samples\n",
    "      folds: number of CV folds\n",
    "    \n",
    "    Returns:\n",
    "      results: list of dicts with mean, std, and 95% CI per metric per model\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "    all_results = []\n",
    "    \n",
    "    for spec in modelspecs:\n",
    "        # Collect per-fold metric scores\n",
    "        fold_scores = {m: [] for m in ['accuracy','precision','recall','auc']}\n",
    "        \n",
    "        for train_idx, val_idx in skf.split(np.zeros(len(y)), y):\n",
    "            # Build fresh model for each fold\n",
    "            model = build_fn(\n",
    "                base_model=spec['base_model'],\n",
    "                top_units=spec['top_units'],\n",
    "                learning_rate=spec['lr']\n",
    "            )\n",
    "            # Create generators for this fold\n",
    "            train_gen, val_gen = X_generator(train_idx, val_idx)\n",
    "            \n",
    "            # Train & evaluate on validation split only\n",
    "            model.fit(train_gen, epochs=5, verbose=0)\n",
    "            scores = model.evaluate(val_gen, verbose=0)\n",
    "            names = model.metrics_names\n",
    "            \n",
    "            # Store each metric value\n",
    "            for name, value in zip(names, scores):\n",
    "                if name in fold_scores:\n",
    "                    fold_scores[name].append(value)\n",
    "        \n",
    "        # Compute mean, std, and 95% CI for each metric\n",
    "        summary = {'model': spec['name']}\n",
    "        for name, vals in fold_scores.items():\n",
    "            arr = np.array(vals)\n",
    "            mean = arr.mean()\n",
    "            std = arr.std(ddof=1)\n",
    "            ci95 = 1.96 * std / np.sqrt(folds)\n",
    "            summary[f'{name}_mean'] = mean\n",
    "            summary[f'{name}_std'] = std\n",
    "            summary[f'{name}_ci95'] = ci95\n",
    "        \n",
    "        all_results.append(summary)\n",
    "    \n",
    "    return all_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957b4581",
   "metadata": {},
   "source": [
    "#### 5.2 - Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca64c13",
   "metadata": {},
   "source": [
    "##### 5.2.1 Bar‐Chart Visualization of Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39529400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you’ve run compare_model_performance and have a DataFrame `df` indexed by model\n",
    "import matplotlib.pyplot as plt  # For plotting\n",
    "\n",
    "# List the metrics you want to visualize\n",
    "metrics = [c for c in df_metrics.columns if c != 'loss']  # e.g. ['accuracy','precision','recall','auc']\n",
    "\n",
    "for metric in metrics:\n",
    "    # Create a new figure for each metric\n",
    "    plt.figure()\n",
    "    \n",
    "    # Plot a bar chart: x = model names, y = metric values\n",
    "    df_metrics[metric].plot(\n",
    "        kind='bar'           # Bar chart\n",
    "    )\n",
    "    \n",
    "    # Add titles and labels\n",
    "    plt.title(f'Model Comparison: {metric.capitalize()}')  # Chart title\n",
    "    plt.xlabel('Model')                                   # x-axis label\n",
    "    plt.ylabel(metric.capitalize())                       # y-axis label\n",
    "    \n",
    "    plt.tight_layout()  # Adjust subplot to fit labels\n",
    "    plt.show()          # Render the plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429f5615",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### 6 - Test on the held-out test set\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96afca1f",
   "metadata": {},
   "source": [
    "---\n",
    "#### 6.1 - Select the trained model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e8d1f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "289/289 [==============================] - 104s 358ms/step - loss: 0.2754 - accuracy: 0.9005 - precision: 0.9923 - recall: 0.9066 - auc: 0.7047\n"
     ]
    }
   ],
   "source": [
    "# 0. Select the trained model for evaluation\n",
    "chosen_model = mobilenet_model  # Replace with your best‐performing model\n",
    "\n",
    "# 1. Evaluate on the test set; returns a list [loss, accuracy, precision, recall, auc] (or whatever metrics you compiled)\n",
    "test_metrics = chosen_model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80aba175",
   "metadata": {},
   "source": [
    "---\n",
    "#### 6.2 - Print a classification report \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecbfc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2754\n",
      "Test Accuracy: 0.9005\n",
      "Test Precision: 0.9923\n",
      "Test Recall: 0.9066\n",
      "Test Auc: 0.7047\n",
      "149/289 [==============>...............] - ETA: 51s"
     ]
    }
   ],
   "source": [
    "# 2. Retrieve the metric names in the same order as test_metrics\n",
    "metric_names = chosen_model.metrics_names  # e.g., ['loss','accuracy','precision','recall','auc']\n",
    "\n",
    "# 3. Print each metric name and its value\n",
    "for name, value in zip(metric_names, test_metrics):\n",
    "    print(f\"Test {name.capitalize()}: {value:.4f}\")  # Nicely formatted output\n",
    "\n",
    "# 4. Predict probabilities on each test image\n",
    "probabilities = chosen_model.predict(test_generator)  # Shape: (num_samples, 1)\n",
    "\n",
    "# 5. Convert probabilities to binary class labels using a 0.5 threshold\n",
    "pred_labels = (probabilities > 0.5).astype(int).ravel()\n",
    "\n",
    "# 6. Print a classification report (precision, recall, F1 for each class)\n",
    "print(classification_report(\n",
    "    test_generator.classes,    # True labels from the generator\n",
    "    pred_labels,               # Predicted labels\n",
    "    target_names=['menu','non_menu']  # Ensure the order matches class_indices\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f66381",
   "metadata": {},
   "source": [
    "---\n",
    "#### 6.3 - Compute and plot the confusion matrix\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee47c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cm = confusion_matrix(test_generator.classes, pred_labels)  # Compute the matrix\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,               # Annotate cells with counts\n",
    "    fmt='d',                  # Integer format\n",
    "    cmap='Blues',             # Color map\n",
    "    xticklabels=['menu','non_menu'],  # Predicted labels\n",
    "    yticklabels=['menu','non_menu']   # True labels\n",
    ")\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix on Test Set')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d1fd15",
   "metadata": {},
   "source": [
    "---\n",
    "#### 6.4 -  Save the trained model for later use\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2682cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 8. (Optional)\n",
    "save_dir = r\"C:\\Users\\leomo\\Desktop\\DATA SCIENTIST CAREER\\PROJECTS\\Menu_Classifier\\saved_models\"\n",
    "os.makedirs(save_dir, exist_ok=True)  # Create folder if it doesn't exist\n",
    "save_path = os.path.join(save_dir, \"menu_classifier2.h5\")\n",
    "chosen_model.save(save_path)  \n",
    "print(f\"Model saved to: {save_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
